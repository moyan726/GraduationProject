# 毕业设计分阶段实施计划表

**项目名称**：基于 Spark 的多品类电商用户行为分析系统设计与性能优化研究  
**指导教师**：[您的名字/AI 导师]  
**学生姓名**：[待填]  
**专业**：数据科学与大数据技术  

---

## 第一部分：项目概述与目标重申

### 1. 核心研究问题
本项目旨在解决在大规模电商点击流数据（REES46 数据集，约 2.85 亿条记录）环境下，如何构建高效、可扩展的分析架构。重点解决以下三个核心问题：
- **工程性能**：对比行式与列式存储性能，治理分布式计算中的数据倾斜。
- **业务深度**：构建从转化漏斗、留存矩阵到 RFM 用户价值分层的全方位指标体系。
- **算法应用**：利用 K-Means 聚类实现数据驱动的用户分群，而非单一的静态规则。

### 2. 预期目标与创新点
- **双轨开发策略**：通过 Track A（样本验证）与 Track B（性能工程）解耦业务逻辑与算力瓶颈。
- **多维性能对比**：量化评估 Parquet 存储、谓词下推及加盐优化对海量数据的加速比。
- **端到端闭环**：实现从原始日志到分布式数仓，再到关系型数据库服务层及 Power BI 可视化的完整链路。

### 3. 最终交付成果
1. **完整代码库**：包含 Spark ETL 脚本、Hive 建表 SQL、K-Means 模型代码、MySQL 导出任务。
2. **实验数据集**：清洗后的 Parquet 格式数据集及 ADS 层结果表。
3. **可视化作品**：Power BI 交互式分析仪表盘文件（.pbix）。
4. **毕业论文**：一份不少于 1.5 万字的学术论文（含实验数据与图表）。

---

## 第二部分：分阶段详细实施计划（共 16 周）

### 第一阶段：数据理解、采集与环境摸底（第 1-3 周）
- **核心任务**：
    1. 下载并部署 REES46 数据集，配置 Hadoop/Spark 基础开发环境。
    2. **Track A 启动**：抽取单月数据（如 2019-10）进行探索性分析（EDA）。
    3. 统计字段缺失率、异常值比例，并完成数据字典编写。
    4. 阅读并总结 3-5 篇关于“电商转化漏斗”与“Spark 性能调优”的核心文献。
- **所需技术与工具**：
    - Python 3.8+ / PySpark / Pandas
    - VS Code / Jupyter Notebook
- **预期产出与验收标准**：
    - **产出**：一份详细的 EDA 报告（PDF）及初步的数据清洗规则文档。
    - **验收标准**：明确数据集关键字段的业务含义，识别出 brand/category_code 的缺失分布。

### 第二阶段：数仓分层设计与 ETL 实施（第 4-6 周）
- **核心任务**：
    1. 设计 ODS → DWD → DWS → ADS 的四层数仓架构。
    2. 编写 Spark 任务实现 DWD 层的保真清洗（类型转换、保留 NULL、dt 分区）。
    3. **存储基准实验**：对比全表扫描、单列聚合在 CSV 与 Parquet 格式下的耗时差异。
- **所需技术与工具**：
    - Apache Spark / Hive / HDFS
- **预期产出与验收标准**：
    - **产出**：DWD/DWS 建表 SQL 脚本，以及存储格式性能对比测试表。
    - **验收标准**：DWD 层数据能够支持后续所有分析模块，Parquet 分区策略生效。

### 第三阶段：核心指标体系与业务分析实现（第 7-9 周）
- **核心任务**：
    1. **转化漏斗**：计算 Session 与 User 两个维度的转化链路，识别流失瓶颈。
    2. **留存分析**：构建 Cohort 矩阵，分析 2019-10 批次用户在后续月份的周留存率。
    3. 汇总日/周/月的 ADS 层指标表。
- **所需技术与工具**：
    - Spark SQL / Hive
- **预期产出与验收标准**：
    - **产出**：核心分析模块的代码实现，以及导出的 CSV 结果集。
    - **验收标准**：漏斗指标逻辑严密，留存矩阵计算准确。

### 第四阶段：高级建模与性能深度优化（第 10-12 周）
- **核心任务**：
    1. **RFM+KMeans**：计算用户 R/F/M 指标，通过肘部法确定最佳 K 值，运行聚类模型。
    2. **数据治理**：针对计算缓慢的品牌维度，实施“两阶段聚合加盐”优化。
    3. 实施广播连接（Broadcast Join）优化维表关联。
- **所需技术与工具**：
    - PySpark MLlib (K-Means) / StandardScaler
- **预期产出与验收标准**：
    - **产出**：聚类模型评估图表（轮廓系数图）、倾斜治理前后性能对比图。
    - **验收标准**：K-Means 聚类效果具有业务解释性，倾斜任务加速比 > 50%。

### 第五阶段：可视化交付、验证与论文终稿（第 13-16 周）
- **核心任务**：
    1. **Serving 层构建**：将 ADS 结果表从 Hive 导出至 MySQL。
    2. **Power BI 看板**：设计总览、漏斗、留存、分群四个页面的交互式仪表盘。
    3. 撰写毕业论文全文，核对书籍映射知识点，准备答辩演示。
- **所需技术与工具**：
    - MySQL / Power BI / LaTeX (或 Word)
- **预期产出与验收标准**：
    - **产出**：.pbix 报表文件、毕业论文定稿、答辩 PPT。
    - **验收标准**：论文符合学术规范，可视化看板展示逻辑清晰，代码可复现。

---

## 第三部分：关键技术路线详解

### 1. 双轨数据策略（Track A & B）
- **实现建议**：在代码中使用参数控制数据源。Track A 模式下使用 `limit(100000)` 或 `filter(dt='2019-10-01')` 快速验证逻辑，Track B 模式下调用全量数据并在集群运行性能监控。

### 2. 数据倾斜治理路径
- **诊断**：查看 Spark UI，识别 `Max Duration` 与 `Median Duration` 差异巨大的 Task。
- **治理**：对于品牌聚合，先通过 `brand_salt = concat(brand, rand(0, 19))` 打散，再进行第一阶段聚合。

### 3. RFM 特征工程
- **偏度处理**：M 指标（金额）通常呈强右偏分布，必须先执行 `log(x + 1)` 变换，否则聚类中心会被极端高价值用户拉偏。

---

## 第四部分：风险管理与时间缓冲

| 风险描述 | 影响等级 | 应对措施 | 缓冲时间 |
| :--- | :--- | :--- | :--- |
| **计算资源溢出 (OOM)** | 高 | 强制采用分区加载策略，必要时在单月数据上完成全流程，全量仅跑核心指标。 | 1 周 |
| **聚类模型效果不佳** | 中 | 调整特征选择，尝试不同的标准化方法，或引入“人工规则+模型修正”混合方案。 | 1 周 |
| **Power BI 连接瓶颈** | 低 | 采用 MySQL 预聚合宽表模式，而非在 Power BI 内部执行复杂 DAX 计算。 | 0.5 周 |

---

## 第五部分：每周例行检查点建议

- **周一上午**：提交上周工作总结（含关键代码片段、报错记录、实验截图）。
- **周三下午**：技术难点突破（如 K 值确定、SQL 逻辑校验）。
- **周五下午**：下周计划确认与论文对应小节初稿提交。

**导师提醒**：请严格遵守时间节点，尤其是第 6 周的“数仓分层”和第 12 周的“聚类分析”，这两个节点是论文深度与工程量的重要体现。
