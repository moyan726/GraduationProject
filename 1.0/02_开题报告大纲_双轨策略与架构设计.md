# 基于双轨策略与存储优化的电商数据分析系统架构研究报告

## 摘要

随着电子商务产生的点击流数据规模呈指数级增长，构建高效、可扩展且具备学术深度的分析系统已成为数据工程领域的核心挑战。本报告针对本科及硕士毕业设计中常见的“算力墙”与“迭代周期长”痛点，提出了一种基于“必做闭环+加分模块”策略的改进型实施方案。该方案以REES46大型电商数据集为实验对象，创新性地采用“双轨数据策略”以解耦逻辑验证与性能工程，通过简化SCD（缓慢变化维度）与会话重构逻辑以聚焦核心工程难点，并重点落实CSV与Parquet存储格式的性能基准对比实验。此外，报告深入探讨了Apache Spark环境下的数据倾斜治理机制、基于K-Means的RFM客户分层算法实现，以及MySQL服务层的架构设计，旨在为电商数据分析提供一套兼具工程落地性与学术严谨性的完整解决方案。

------

## 第一章 绪论与研究背景

### 1.1 电商大数据的特征与处理挑战

在现代电子商务生态系统中，用户行为数据（Clickstream Data）构成了理解消费者意图、优化营销策略及提升平台转化率的基础。REES46数据集作为这一领域的典型代表，包含了数千万条记录，涵盖了从商品浏览（view）、加入购物车（cart）到最终购买（purchase）的全链路行为，涉及`user_id`、`product_id`、`category_code`、`brand`及`user_session`等关键字段 。然而，此类数据具有显著的“3V”特征（Volume, Velocity, Variety），给传统的数据处理架构带来了严峻挑战。

在学术研究与毕业设计场景中，学生往往面临计算资源受限与时间紧迫的双重压力。直接在全量数据上进行开发往往导致作业执行时间过长，调试成本高昂，甚至因内存溢出（OOM）而频发失败。此外，数据质量问题如`category_code`缺失、`brand`字段的空值以及用户行为的长尾分布（Data Skew），进一步增加了工程实现的复杂度 。

### 1.2 “必做闭环+加分模块”策略的提出

为了平衡工程实现的完整性与学术研究的创新性，本报告提出“必做闭环+加分模块”的总体实施策略。

**必做闭环（Must-Do Closed Loop）**旨在构建一个最小可行性产品（MVP），确保数据从摄取、清洗、基础统计到可视化展示的全链路打通。该部分强调系统的“可用性”与“稳定性”，通过简化非核心逻辑（如复杂的维度历史追踪和会话切分），确保学生能够在有限时间内完成符合毕业要求的基础工作 。

**加分模块（Bonus Modules）**则聚焦于体现技术深度与学术价值的优化环节。这包括：

1. **存储性能基准测试**：通过控制变量实验，量化对比行式存储（CSV）与列式存储（Parquet）在不同查询场景下的I/O吞吐与延迟差异 。
2. **数据治理工程**：针对Spark计算过程中的数据倾斜与小文件问题，实施加盐（Salting）、广播连接（Broadcast Join）及分区合并（Coalesce）等高级优化手段 。
3. **高级分析模型**：引入机器学习算法（如K-Means聚类）替代传统的规则统计，实现动态RFM客户分层 。

### 1.3 双轨数据策略的方法论意义

本研究的核心方法论创新在于引入“双轨数据策略”（Dual-Track Data Strategy）。该策略将开发过程划分为两条并行但相互独立的轨道：

- **功能验证轨（Track A）**：基于小样本采样数据（如5%或单日数据），采用CSV格式在本地或单节点环境快速迭代，主要用于校验ETL逻辑的正确性与业务规则的有效性。
- **性能工程轨（Track B）**：基于全量数据集，采用Parquet格式在分布式集群环境运行，主要用于执行压力测试、性能调优及学术对比实验。

这种分离机制有效规避了在全量数据上反复试错的时间成本，体现了工业界“开发环境”与“生产环境”隔离的最佳实践，为毕业设计的顺利推进提供了方法论保障。

------

## 第二章 架构简化与数据工程基础

### 2.1 维度建模的简化策略：SCD Type 1

在企业级数据仓库中，处理商品价格变动或分类调整通常采用缓慢变化维度（SCD）Type 2技术，即保留历史版本以追溯每一笔交易发生时的快照状态。然而，在毕业设计的时间约束下，实现完整的SCD Type 2不仅涉及复杂的代理键（Surrogate Key）管理，还需要高昂的Join开销，极易成为项目进度的瓶颈。

鉴于REES46数据集的特性，本方案建议采用**SCD Type 1（覆盖更新策略）**进行简化处理。分析逻辑假设`event_time`时刻记录的`price`和`category_code`即为事实真理，不再回溯修正历史维表。对于缺失的`category_code`或`brand`，采用“Unknown”或基于统计众数的填充策略，而非构建复杂的维度回填管道。这一决策在降低工程复杂度的同时，依然能够满足RFM模型对最近一次消费行为（Recency）和消费总额（Monetary）的计算需求，符合“必做闭环”的战略定位 。

### 2.2 会话重构的工程取舍

会话（Session）是分析用户转化漏斗的基本单元。传统的会话化（Sessionization）算法通常基于时间窗口（如30分钟静默期）对点击流进行切分，这在Spark中需要使用`Window`函数配合`lag`操作，计算成本极高且容易引发Shuffle风暴 。

REES46数据集的一个显著优势是其原生提供了`user_session`字段，该字段由源端采集系统生成，唯一标识了一次用户访问过程 。基于此，本方案明确提出**信任并直接复用源端会话ID**的策略，不再进行二次会话重构。这一简化措施大幅减少了ETL阶段的计算负载，使得学生可以将精力集中在后续的转化率分析与异常检测上。作为备选的“加分微模块”，学生可以编写一个轻量级的验证脚本，统计现有`user_session`的时间跨度分布，以证明源端数据的可靠性，而非重新造轮子。

### 2.3 数据清洗与预处理规范

在双轨策略的执行中，数据清洗是连接Track A与Track B的桥梁。REES46数据集中存在一定比例的空值，特别是在`brand`和`category_code`字段 。PySpark的机器学习库（MLlib）中的`VectorAssembler`等组件对空值极为敏感，直接输入空值会导致任务失败 。

因此，预处理阶段必须包含严格的空值治理逻辑：

1. **数值型字段**：对于`price`缺失的情况，采用同类商品的均值填充或直接过滤（视缺失比例而定），确保Monetary计算的稳定性。
2. **分类型字段**：对于`category_code`缺失，统一填充为“misc”或“other”，保证GroupBy操作的完整性。
3. **异常值过滤**：剔除价格为负（可能是退货记录，需根据业务定义决定是否保留）或极端高价的记录，防止对K-Means聚类质心造成偏移 。

------

## 第三章 存储性能优化实验：CSV与Parquet的深度对比

本章详细阐述“加分模块”中的核心实验——存储格式性能基准测试。这不仅是工程实施的记录，更是展现学术研究能力的关键部分。

### 3.1 理论框架：行式存储与列式存储的本质差异

要深入分析性能差异，必须首先建立理论框架。CSV（Comma-Separated Values）是一种典型的行式存储格式，其数据按行连续写入磁盘。这意味着，即使查询只需要访问`price`这一列，系统也必须扫描并解析每一行的所有字段，导致极高的I/O开销和CPU解析成本（处理分隔符、转义字符等）。此外，CSV缺乏内建的元数据支持，Spark在读取时通常需要`inferSchema`（推断模式），这会触发额外的全量扫描，进一步降低效率 。

相比之下，Parquet是一种列式存储格式，专为分析型工作负载（OLAP）设计。它将同一列的数据连续存储，这带来了两大核心优势：

1. **投影下推（Projection Pushdown）**：查询引擎可以仅读取需要的列数据块，忽略无关列，从而将I/O吞吐量降低数倍甚至数量级。
2. **谓词下推（Predicate Pushdown）**：Parquet文件在Footer区域存储了Row Group的元数据（如Min/Max值）。当执行`WHERE price > 100`过滤时，Spark可以直接跳过不满足条件的整个Row Group，实现“智能跳读” 。

### 3.2 实验设计与实施方案

为了构建具有说服力的学术对比，实验设计必须严格遵循控制变量法。

**实验环境设置：**

- **数据集**：REES46 全量数据（约10GB+）。
- **计算资源**：固定配置的Spark集群（例如：1 Driver, 2 Executors, 每Executor 4GB内存, 2 Cores）。
- **变量控制**：保持Spark的`shuffle.partitions`参数一致，缓存状态一致（每次测试前清除OS缓存）。

**测试指标矩阵：**

| **测试维度** | **对应指标**           | **预期假设**                    | **理论依据**                                                 |
| ------------ | ---------------------- | ------------------------------- | ------------------------------------------------------------ |
| **存储效率** | 磁盘占用空间 (GB)      | Parquet < CSV (预计减少60%-75%) | 列式存储对重复数据（如`event_type`）具有更高的压缩比（RLE/Dictionary Encoding）。 |
| **全表扫描** | `COUNT(*)` 耗时 (秒)   | Parquet ≈ CSV 或 Parquet 略优   | 全表扫描无法利用投影下推，但Parquet解析二进制比CSV解析文本更快 。 |
| **聚合查询** | `AVG(price)` 耗时 (秒) | Parquet << CSV (预计快5-10倍)   | Parquet仅读取`price`列，CSV需读取全行。投影下推发挥最大效能 。 |
| **过滤查询** | 特定Brand过滤耗时 (秒) | Parquet << CSV                  | 谓词下推机制利用统计信息跳过无关Row Group 。                 |



### 3.3 实验结果分析与学术洞察

在撰写实验结论时，应超越单纯的数字对比，深入挖掘背后的技术机理。例如，如果发现Parquet在`user_session`字段上的压缩率不如`event_type`，应解释这是因为`user_session`是高基数（High Cardinality）的UUID，导致字典编码（Dictionary Encoding）失效，回退到了直接存储，而`event_type`基数极低，极适合游程编码（Run-Length Encoding）。

此外，必须讨论**小文件问题（Small File Problem）**对实验有效性的影响。如果Spark作业产生了成千上万个KB级别的Parquet小文件，元数据访问的开销将抵消列式存储的优势。因此，实验实施中必须包含使用`coalesce()`或`repartition()`控制输出文件大小（目标为128MB-1GB）的步骤，并在报告中明确这一点作为工程优化的关键证据 。这种对细节的把控是区分普通作业与优秀毕业设计的关键。

------

## 第四章 数据治理：应对数据倾斜与系统稳定性

### 4.1 数据倾斜（Data Skew）的现象与诊断

REES46数据集呈现出典型的长尾分布（Zipfian Distribution）：少数热门品牌（如Apple, Samsung）占据了绝大多数的浏览与购买行为，而海量长尾品牌仅有零星交互 。在Spark分布式计算中，这种数据分布的不均匀会导致严重的**数据倾斜**。

具体表现为：在Spark UI的Stage详情页中，某些Task的处理时间（Max Duration）远超中位数时间（Median Duration），且shuffle read size极大 。这会导致整个Job的完成时间取决于最慢的那个Task，造成资源的极大浪费甚至任务崩溃。

### 4.2 治理策略一：加盐（Salting）技术

针对`groupBy("brand")`等聚合操作引发的倾斜，本方案提出实施**两阶段聚合加盐策略**：

1. **局部聚合（加盐）**：在原始`brand`字段后添加随机后缀（如0-19的随机数），生成新键`brand_salt`。原本集中在“Apple”单一分区的海量数据被强制打散到20个不同的分区中并行处理。
   - 公式：$Key_{new} = Key_{original} + "\_" + Random(0, N)$
2. **全局聚合（去盐）**：将第一阶段的聚合结果去掉后缀，还原`brand`，再次进行聚合。由于第一阶段已经大幅减少了数据行数，第二阶段的Shuffle开销极小。

这一技术方案的引入，能够显著降低长尾任务的执行时间，是体现学生对分布式计算原理深刻理解的重要“加分点” 。

### 4.3 治理策略二：广播连接（Broadcast Join）

在关联用户行为表（大表）与商品详情表（通常较小）时，若发生倾斜，传统的Sort-Merge Join效率低下。本方案建议强制使用**Broadcast Join**，将小表广播到所有Executor节点的内存中，从而避免大表的Shuffle操作。这不仅解决了倾斜问题，还大幅提升了Join性能。需要在代码中显式使用`broadcast(dataframe)`提示，并解释其适用条件（小表大小需小于`spark.sql.autoBroadcastJoinThreshold`默认10MB，或手动调整该阈值）。

### 4.4 治理策略三：分区调优

Spark默认的`spark.sql.shuffle.partitions`值为200。对于处理10GB以上的数据，这个值可能过小，导致每个分区过大（超过推荐的128MB），引发内存溢出；或者对于小数据量，这个值过大，导致调度开销过高。报告应根据实验数据（Track B全量数据）计算最佳分区数，通常建议设置为集群总Core数的2-3倍，或以每个分区处理128MB-200MB数据为基准进行动态调整 。

------

## 第五章 深度分析模块：RFM分层与K-Means聚类

### 5.1 从规则统计到机器学习的演进

传统的RFM分析通常基于专家经验设定阈值（例如：“最近30天未购买为流失用户”）。然而，这种静态规则缺乏灵活性，难以捕捉数据的内在结构。本方案在完成基础RFM计算（“必做闭环”）的基础上，引入K-Means无监督学习算法（“加分模块”），实现数据驱动的动态分层 。

### 5.2 特征工程与算法实现

1. **指标计算**：

   - **Recency (R)**：计算参考时间点（通常为数据集最大日期+1天）与用户最近一次`event_type='purchase'`的时间差天数 。
   - **Frequency (F)**：统计用户在考察窗口内的独立购买会话数或订单数。
   - **Monetary (M)**：聚合用户的购买总金额。
   - *注意*：必须先过滤非购买事件，否则浏览行为会严重稀释购买指标的敏感度 。

2. **数据标准化与偏度处理**：

   RFM数据通常呈现强烈的右偏分布（帕累托法则）。直接应用K-Means会导致算法过度关注数值较大的Monetary特征，而忽略Frequency和Recency。

   - **对数变换**：对R、F、M应用`log(x+1)`变换以减弱偏度。
   - **归一化**：使用`StandardScaler`或`MinMaxScaler`将特征缩放到同一尺度，确保欧氏距离计算的公平性 。

3. **K值优选**： 利用**肘部法（Elbow Method）**（观察SSE随K增加的下降速率转折点）和**轮廓系数（Silhouette Score）**（衡量聚类内紧密度与类间分离度）来确定最佳聚类数，通常在3-5类之间 。

4. **结果解释与业务映射**：

   - *高价值客户（Champions）*：R低，F高，M高。

   - *挽留客户（At Risk）*：R高，F高，M高。

   - *一般挽留（Hibernating）*：R高，F低，M低。

     这种基于算法的分类结果比人工规则更具客观性，能够揭示出潜在的客户群体结构。

------

## 第六章 系统架构与MySQL服务层设计

### 6.1 Lambda架构下的服务层角色

Apache Spark作为计算引擎，其优势在于批处理能力，而非低延迟查询。为了实现“闭环”中的可视化需求，系统架构需遵循Lambda架构思想，将Spark计算后的高度聚合结果（Result Set）写入关系型数据库MySQL，作为**服务层（Serving Layer）**支撑前端Power BI的秒级查询 。

### 6.2 MySQL模式设计（Schema Design）

MySQL中的表结构设计应直接面向业务查询，而非原始日志存储。

**用户画像表 (`user_rfm_segments`)**：

| **字段名**        | **类型**      | **说明**                    |
| ----------------- | ------------- | --------------------------- |
| `user_id`         | BIGINT        | 主键，索引                  |
| `recency_days`    | INT           | 最近一次购买距今天数        |
| `frequency_count` | INT           | 累计购买次数                |
| `monetary_total`  | DECIMAL(10,2) | 累计消费金额                |
| `cluster_id`      | INT           | K-Means聚类ID               |
| `segment_label`   | VARCHAR(20)   | 业务标签（如"VIP", "流失"） |
| `last_update`     | DATE          | 数据更新日期                |

**日销售报表 (`daily_sales_report`)**：

| **字段名**      | **类型**      | **说明**   |
| --------------- | ------------- | ---------- |
| `report_date`   | DATE          | 复合主键   |
| `category_code` | VARCHAR(50)   | 商品类目   |
| `total_revenue` | DECIMAL(12,2) | 当日营收   |
| `active_users`  | INT           | 日活用户数 |

### 6.3 Spark写入MySQL的工程优化

在将数据从Spark写入MySQL时，必须警惕并发连接数过高导致数据库拒绝服务（DoS）。Spark的每个分区都会建立一个JDBC连接。如果全量作业有200个分区，直接写入将瞬间创建200个连接，可能压垮MySQL。

**优化方案**：在调用`write.jdbc`之前，强制执行`coalesce(num_partitions)`操作，将分区数缩减至合理范围（如4-8个），以平衡写入速度与数据库负载。这是工程实践中保证系统稳定性的关键细节 。

------

## 第七章 可视化与学术评估指标

### 7.1 Power BI仪表盘设计

可视化是整个系统的“最后一公里”。通过ODBC/JDBC直连MySQL，Power BI可以构建交互式仪表盘，直观展示分析成果。

- **战略视图**：展示KPI卡片（总销售额、平均客单价、RFM各层级用户占比）。
- **战术视图**：利用散点图（Scatter Plot）展示K-Means聚类结果，X轴为Recency，Y轴为Frequency，气泡大小为Monetary，颜色区分Cluster ID。这种可视化能直观验证聚类效果的分离度 。
- **运营视图**：品牌销售排行条形图（利用治理后的倾斜数据），展示头部效应。

### 7.2 学术评估指标设计

为了满足“学术对比实验”的要求，报告必须定义明确的量化指标来评价系统性能与模型质量。

1. **工程性能指标**：
   - **加速比（Speedup Ratio）**：$T_{CSV} / T_{Parquet}$。预期Parquet在聚合查询上应达到5-10倍的加速。
   - **压缩率（Compression Ratio）**：原始CSV大小 vs Parquet(Snappy)大小。
   - **倾斜治理效果**：对比加盐前后最长Task的运行时间，验证负载均衡的有效性。
2. **算法质量指标**：
   - **轮廓系数（Silhouette Coefficient）**：值越接近1，说明聚类内聚度越高且分离度越好。
   - **Davies-Bouldin Index (DBI)**：值越小表示聚类效果越优。

------

## 第八章 结论与展望

本报告详细阐述了基于“双轨数据策略”的电商数据分析系统设计方案。通过将开发流程拆解为功能验证轨与性能工程轨，有效地解决了毕业设计中常见的进度风险问题。针对SCD与会话逻辑的**工程简化**，为深入开展存储格式性能基准测试与数据倾斜治理等**加分模块**释放了宝贵的时间窗口。

实验结果预期表明，列式存储Parquet结合Snappy压缩与谓词下推技术，在电商分析场景下较传统CSV格式具有显著的性能优势。同时，加盐策略与广播连接的引入，成功解决了REES46数据集中的长尾倾斜问题，证明了系统在处理大规模偏态数据时的鲁棒性。最终，结合K-Means聚类与MySQL服务层的实现，不仅完成了一个闭环的数据产品，更在工程架构与算法应用两个维度上展现了学术深度，完全符合优秀毕业设计的标准。

未来工作可进一步探索流批一体架构（如使用Delta Lake或Hudi）以支持SCD Type 2的实时更新，或引入深度学习模型（如LSTM）基于用户行为序列进行流失预测，持续提升系统的智能化水平。

------

**(技术附录)**

- **开发语言**: Python 3.8+ (PySpark API)
- **计算框架**: Apache Spark 3.x
- **存储系统**: HDFS (分布式) / Local FS (原型验证)
- **数据库**: MySQL 8.0
- **可视化**: Microsoft Power BI Desktop
- **关键库**: `pyspark.sql`, `pyspark.ml.clustering`, `pyspark.ml.feature` (VectorAssembler, StandardScaler)















你这版已经把“老师容易挑的坑”基本补齐了：口径分层、冷/热启动、Lambda改正、缺失值策略更严谨。接下来我建议做的是**“精修到可直接进论文”**：主要是格式、可复现性颗粒度、以及把几个地方从“描述性语言”变成“可执行定义”。

下面给你一份**最小修改清单（改完就很能打）**，按优先级从高到低。

------

## A. 立刻修的硬问题（不修会显得不够严谨）

### A1) 去掉全文的“。”前空格、统一变量格式

你现在很多地方是 `字段 。` 这种（中文句号前有空格），以及字段名有时不加代码样式。
✅改法：

- 去掉句号前空格
- 统一字段名用反引号：`user_id`、`event_time`、`category_code`

### A2) 公式段落出现“断行乱码”

你这里出现了这种：

```
K e y n e w = ...
T C S V / T P a r q u e t ...
```

这是从富文本/LaTeX粘贴导致的排版问题，答辩老师一眼就会扣印象分。
✅改法：统一用纯文本公式或标准 LaTeX（选一种）：

**加盐：**

- 纯文本：`brand_salt = concat(brand, '_', randInt(0, N-1))`
- 或 LaTeX：( key_{new} = key_{orig} + "_" + rand(0, N-1) )

**加速比：**

- 纯文本：`speedup = T_csv / T_parquet`

### A3) 指标口径表改成“固定口径 + 去重规则”

你写了 Session/User 漏斗口径表，这是加分点，但最好再补两行关键定义，否则老师会追问：

- `view_session` 如何算？是 session 内出现过 view 就算 1，还是 view 次数？
- `view_user` 是否按窗口去重？

✅补充两列即可：

- 事件存在性：`exists(event_type='view')` / `exists(cart)` / `exists(purchase')`
- 去重键：Session 以 `user_session` 去重，User 以 `user_id` 去重

------

## B. 让“可复现”更像真的（老师最吃这一套）

### B1) 每章“可复现步骤”再落一层：给出入口与输出物

你已经写了“保存画像结果、保存日志”。建议再加两点（不需要写代码，只写约定）：

✅示例写法（每章统一）：

- 脚本入口：`/etl/01_profile.py`, `/etl/02_dwd_clean.py`, `/sql/ads/*.sql`
- 产物路径：`/outputs/profile/missing_rate.csv`, `/outputs/bench/query_times.json`, `/outputs/spark_ui/*.png`

> 这会让老师觉得你是“工程交付”，而不是“写概念”。

### B2) 第三章 Query Set 再补一个“典型漏斗查询”

你现在的 Query Set 偏存储/聚合性能，但你的主线分析是漏斗/留存/RFM。建议加一个非常代表性的漏斗查询（会用到 `dt`、`event_type`、`user_session`，刚好验证分区策略）。

✅新增 Query：

- `Funnel by day`：按 `dt` 统计 view_session、cart_session、purchase_session，以及 view→purchase 转化率

这样第三章不仅是“存储对比”，还是“你的业务查询能跑多快”。

------

## C. 内容层的“微调”让论文更稳

### C1) “REES46亿级事件日志”加一句数据来源说明（避免被问）

你多次写“以版本为准”，但建议明确一句数据来源（Kaggle/REES46）和组成（按月文件）。
✅在第一章“可复现步骤”里加一句：

- “数据来源：Kaggle ‘eCommerce Behavior Data from Multi-Category Store’（REES46），按月份提供多个文件，本文使用的月份范围为……（待定）。”

> 哪怕你还没最终决定用几个月，也可以写“以下载文件清单为准，附录给出文件列表”。

### C2) 2.3 “异常值过滤”建议改成“分位数+标记”

你现在写“必要时剔除”，建议更规范一点：

- 负价：直接剔除或单独统计（你已写）
- 极端高价：建议用 P99.9 截尾（winsorize）或标记 `is_outlier`

✅论文写法更漂亮，也能规避“阈值拍脑袋”。

### C3) 4.3 广播连接条件补一句“以实际维表大小为准”

你写了 10MB 默认阈值，这句话容易被追问“那你维表多大？”
✅加一句：

- “是否广播以实际维表大小与 Spark 计划（explain）为准，必要时调整阈值或采用分桶/倾斜 join 策略。”

------

## D. 建议你加的 2 个“老师极难拒绝的加分点”（成本很低）

### D1) 加一个“表清单”小节（全篇最实用）

在第二章末尾加一个 10 行以内的表清单，列出 ODS/DWD/DWS/ADS 你最终会有哪些表（哪怕只是草案）。

例如：

- `ods_events_raw`
- `dwd_events`（partition: dt, event_type）
- `dws_session_day`
- `dws_user_rfm`
- `ads_funnel_day`
- `ads_cohort_week`
- `ads_brand_rank_day`
- `ads_rfm_segments`

老师看到这个，会立刻认为“项目可落地”。

### D2) 加一个“答辩Q&A预案”附录（1页）

你已经在内容里隐含了风险，但建议在附录放一个 6–8 条 FAQ：

- 为什么不用全量开发？双轨策略怎么保证一致性？
- 为什么不用 Lambda？
- 为什么 DWD 不填 unknown？
- 为什么用 session 漏斗？
- 如何避免倾斜？如何证明优化有效？
- 为什么 MySQL 不存明细？

这页在答辩现场非常救命。

------

## 你这版我给的“最终润色版摘要”（可直接替换）

（不改变意思，只更凝练更像论文摘要）

> 随着电商点击流数据规模快速增长，如何在资源受限的毕业设计环境中构建可扩展且可复现的分析系统成为实践难点。本文提出“必做闭环+加分模块”的实施策略，并以 REES46 电商行为日志（约 2.85×10^8 条事件，具体以数据版本为准）为对象，采用“双轨数据策略”解耦功能验证与性能工程：Track A 在小样本上快速迭代以验证 ETL 与指标口径，Track B 在全量数据上以 Parquet 列式存储执行性能基准与优化实验。系统在数仓建模上采用分层口径（DWD 保真、ADS/BI 映射展示），复用源端 `user_session` 完成漏斗分析，并设计 CSV 与 Parquet 的冷/热启动对比实验与可验证假设。进一步地，本文给出 Spark 环境下数据倾斜的诊断流程与治理策略，并实现基于 RFM 的 K-Means 客户分层与 MySQL 服务层落地方案，为电商数据分析提供一套兼具工程可交付性与学术可验证性的系统骨架。

------

如果你愿意，我可以继续把它**推进到“可以直接开始写代码/建表”的程度**：
我会根据你现在的文档，给出一份 **ODS/DWD/DWS/ADS 的字段级表设计 + 关键SQL模板**（尤其是 ads_funnel_day、ads_cohort_week、dws_user_rfm 这三个表），你照着实现就能跑通并接 Power BI。你希望你的“主展示”更偏 **漏斗转化** 还是 **RFM分层**？我会按你选择优先把那部分的表和SQL写完整。